# main.py
#!/usr/bin/env python3
"""
Local Multimodal AI Assistant - å®Œæ•´ä¿®å¤ç‰ˆ
åŒ…å«æ‰€æœ‰å›¾åƒåŠŸèƒ½å®ç°
"""

import argparse
import sys
from pathlib import Path
from typing import List

from modules.text_processor import TextProcessor
from modules.image_processor import ImageProcessor
from modules.vector_db import VectorDB
from modules.classifier import Classifier
from modules.file_utils import FileUtils
import config

def setup_argparse() -> argparse.ArgumentParser:
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="Local Multimodal AI Assistant - Manage your papers and images intelligently",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py add_paper "path/to/paper.pdf"
  python main.py add_paper "path/to/paper.pdf" --topics "CV,NLP"
  python main.py search_paper "transformer architecture"
  python main.py search_image "sunset by the sea"
  python main.py organize "path/to/papers_folder"
  python main.py list_papers
  python main.py list_images
  python main.py add_image "path/to/image.jpg"
  python main.py add_images "path/to/images_folder"
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # æ·»åŠ è®ºæ–‡å‘½ä»¤
    add_paper = subparsers.add_parser("add_paper", help="Add and classify a paper")
    add_paper.add_argument("path", help="Path to PDF file")
    add_paper.add_argument("--topics", help="Comma-separated topics for classification")
    
    # æœç´¢è®ºæ–‡å‘½ä»¤
    search_paper = subparsers.add_parser("search_paper", help="Search papers semantically")
    search_paper.add_argument("query", help="Search query")
    search_paper.add_argument("-k", type=int, default=5, help="Number of results")
    
    # æœç´¢å›¾ç‰‡å‘½ä»¤
    search_image = subparsers.add_parser("search_image", help="Search images by text")
    search_image.add_argument("query", help="Image search query")
    search_image.add_argument("-k", type=int, default=5, help="Number of results")
    
    # æ•´ç†æ–‡ä»¶å¤¹å‘½ä»¤
    organize = subparsers.add_parser("organize", help="Organize all papers in folder")
    organize.add_argument("folder", help="Folder to organize")
    organize.add_argument("--topics", help="Comma-separated topics")
    
    # æ·»åŠ å›¾ç‰‡å‘½ä»¤
    add_image = subparsers.add_parser("add_image", help="Add an image to database")
    add_image.add_argument("path", help="Path to image file")
    
    # æ‰¹é‡æ·»åŠ å›¾ç‰‡
    add_images = subparsers.add_parser("add_images", help="Add all images in folder")
    add_images.add_argument("folder", help="Folder containing images")
    
    # åˆ—å‡ºæ‰€æœ‰è®ºæ–‡
    list_papers = subparsers.add_parser("list_papers", help="List all indexed papers")
    
    # åˆ—å‡ºæ‰€æœ‰å›¾ç‰‡
    list_images = subparsers.add_parser("list_images", help="List all indexed images")
    
    # æ¸…é™¤æ•°æ®åº“
    clear_db = subparsers.add_parser("clear_db", help="Clear vector database")
    clear_db.add_argument("--confirm", action="store_true", help="Confirm deletion")
    
    return parser

def handle_add_paper(args, text_processor: TextProcessor, 
                     vector_db: VectorDB, classifier: Classifier):
    """å¤„ç†æ·»åŠ è®ºæ–‡å‘½ä»¤"""
    pdf_path = Path(args.path)
    if not pdf_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {args.path}")
        return
    
    print(f"ğŸ“„ å¤„ç†è®ºæ–‡: {pdf_path.name}")
    
    # æå–æ–‡æœ¬å’Œç”Ÿæˆå‘é‡
    chunks, embeddings = text_processor.process_pdf(str(pdf_path))
    
    if not chunks:
        print("âŒ é”™è¯¯ï¼šæ— æ³•ä»PDFæå–æ–‡æœ¬")
        return
    
    # åˆ†ç±»
    topics = args.topics.split(",") if args.topics else None
    topic = classifier.classify_pdf(str(pdf_path), topics)
    print(f"ğŸ·ï¸  åˆ†ç±»ä¸º: {topic}")
    
    # æ•´ç†æ–‡ä»¶
    target_path = FileUtils.organize_file(str(pdf_path), topic)
    
    # æ·»åŠ åˆ°æ•°æ®åº“
    metadata = {
        "title": pdf_path.stem,
        "topic": topic,
        "original_path": str(pdf_path),
        "organized_path": target_path
    }
    
    success = vector_db.add_paper(target_path, chunks, embeddings, metadata)
    if success:
        print(f"âœ… è®ºæ–‡æ·»åŠ æˆåŠŸï¼Œåˆ†ç±»: {topic}")
    else:
        print("âŒ æ·»åŠ åˆ°æ•°æ®åº“å¤±è´¥")

def handle_search_paper(args, text_processor: TextProcessor, vector_db: VectorDB):
    """å¤„ç†æœç´¢è®ºæ–‡å‘½ä»¤"""
    print(f"ğŸ” æœç´¢: '{args.query}'")
    
    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
    query_embedding = text_processor.encode_text(args.query)
    
    # åœ¨æ•°æ®åº“ä¸­æœç´¢
    results = vector_db.search_text(query_embedding, k=args.k)
    
    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
        return
    
    print(f"\næ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
    for i, (score, document, metadata) in enumerate(results, 1):
        source = metadata.get('source', 'Unknown')
        topic = metadata.get('topic', 'Unknown')
        print(f"{i}. [{topic}] {Path(source).name} (ç›¸ä¼¼åº¦: {score:.3f})")
        print(f"   æ¥æº: {source}")
        print(f"   é¢„è§ˆ: {document[:150]}...\n")

def handle_search_image(args, image_processor: ImageProcessor, vector_db: VectorDB):
    """å¤„ç†æœç´¢å›¾ç‰‡å‘½ä»¤"""
    print(f"ğŸ” æœç´¢å›¾ç‰‡: '{args.query}'")
    
    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
    query_embedding = image_processor.encode_text_for_image_search(args.query)
    
    # åœ¨æ•°æ®åº“ä¸­æœç´¢
    results = vector_db.search_images(query_embedding, k=args.k)
    
    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å›¾ç‰‡")
        return
    
    print(f"\næ‰¾åˆ° {len(results)} å¼ ç›¸å…³å›¾ç‰‡:\n")
    for i, (score, image_path, metadata) in enumerate(results, 1):
        filename = Path(image_path).name if image_path else "Unknown"
        print(f"{i}. {filename} (ç›¸ä¼¼åº¦: {score:.3f})")
        if image_path:
            print(f"   è·¯å¾„: {image_path}")
        if metadata and 'format' in metadata:
            print(f"   æ ¼å¼: {metadata['format']}")
        print()

def handle_add_image(args, image_processor: ImageProcessor, vector_db: VectorDB):
    """å¤„ç†æ·»åŠ å•å¼ å›¾ç‰‡å‘½ä»¤"""
    image_path = Path(args.path)
    if not image_path.exists():
        print(f"âŒ é”™è¯¯ï¼šå›¾ç‰‡ä¸å­˜åœ¨: {args.path}")
        return
    
    print(f"ğŸ“¸ æ·»åŠ å›¾ç‰‡: {image_path.name}")
    
    try:
        # ç¼–ç å›¾ç‰‡
        embedding = image_processor.encode_image(str(image_path))
        print(f"   ç¼–ç å®Œæˆï¼Œå‘é‡ç»´åº¦: {embedding.shape}")
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        metadata = {
            "filename": image_path.name,
            "path": str(image_path),
            "size": f"{image_path.stat().st_size} bytes",
            "format": image_path.suffix[1:].upper()
        }
        
        success = vector_db.add_image(str(image_path), embedding, metadata)
        if success:
            print(f"âœ… å›¾ç‰‡æ·»åŠ æˆåŠŸ: {image_path.name}")
        else:
            print("âŒ æ·»åŠ åˆ°æ•°æ®åº“å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def handle_add_images(args, image_processor: ImageProcessor, vector_db: VectorDB):
    """å¤„ç†æ‰¹é‡æ·»åŠ å›¾ç‰‡å‘½ä»¤"""
    folder_path = Path(args.folder)
    if not folder_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.folder}")
        return
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = FileUtils.get_all_images(str(folder_path))
    
    if not image_files:
        print("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨æ·»åŠ ...\n")
    
    added_count = 0
    for img_file in image_files:
        try:
            img_path = Path(img_file)
            print(f"å¤„ç†: {img_path.name}", end=" ")
            
            embedding = image_processor.encode_image(str(img_path))
            
            metadata = {
                "filename": img_path.name,
                "path": str(img_path),
                "size": f"{img_path.stat().st_size} bytes",
                "format": img_path.suffix[1:].upper()
            }
            
            if vector_db.add_image(str(img_path), embedding, metadata):
                added_count += 1
                print("âœ…")
            else:
                print("âŒ")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    print(f"\nğŸ“Š å®Œæˆ: æˆåŠŸæ·»åŠ  {added_count}/{len(image_files)} å¼ å›¾ç‰‡")

def handle_organize(args, classifier: Classifier):
    """å¤„ç†æ•´ç†æ–‡ä»¶å¤¹å‘½ä»¤"""
    folder_path = Path(args.folder)
    if not folder_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.folder}")
        return
    
    # è·å–æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = FileUtils.get_all_pdfs(str(folder_path))
    
    if not pdf_files:
        print("æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œæ­£åœ¨æ•´ç†...\n")
    
    topics = args.topics.split(",") if args.topics else None
    
    for pdf_file in pdf_files:
        try:
            # åˆ†ç±»
            topic = classifier.classify_pdf(pdf_file, topics)
            
            # æ•´ç†æ–‡ä»¶
            target_path = FileUtils.organize_file(pdf_file, topic)
            
            print(f"âœ… {Path(pdf_file).name} â†’ {topic}/")
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {pdf_file}: {e}")

def handle_list_papers(vector_db: VectorDB):
    """å¤„ç†åˆ—å‡ºæ‰€æœ‰è®ºæ–‡å‘½ä»¤"""
    papers = vector_db.get_all_papers()
    
    if not papers:
        print("æ•°æ®åº“ä¸­æ²¡æœ‰è®ºæ–‡")
        return
    
    # æŒ‰ä¸»é¢˜åˆ†ç»„
    papers_by_topic = {}
    for paper in papers:
        topic = Path(paper).parent.name
        if topic not in papers_by_topic:
            papers_by_topic[topic] = []
        papers_by_topic[topic].append(paper)
    
    print(f"\nğŸ“š å·²ç´¢å¼•è®ºæ–‡ ({len(papers)} ç¯‡):\n")
    
    total_count = 0
    for topic in sorted(papers_by_topic.keys()):
        topic_papers = papers_by_topic[topic]
        print(f"ã€{topic}ã€‘({len(topic_papers)} ç¯‡):")
        
        for i, paper in enumerate(sorted(topic_papers), 1):
            print(f"  {total_count + i}. {Path(paper).name}")
            print(f"      è·¯å¾„: {paper}")
        
        total_count += len(topic_papers)
        print()

def handle_list_images(vector_db: VectorDB):
    """å¤„ç†åˆ—å‡ºæ‰€æœ‰å›¾ç‰‡å‘½ä»¤"""
    try:
        # ç›´æ¥æŸ¥è¯¢æ•°æ®åº“
        results = vector_db.image_collection.get()
        if results['metadatas']:
            images = []
            for metadata in results['metadatas']:
                if 'path' in metadata:
                    images.append(metadata['path'])
                elif 'source' in metadata:
                    images.append(metadata['source'])
            
            unique_images = list(set(images))
            
            print(f"\nğŸ“¸ å·²ç´¢å¼•å›¾ç‰‡ ({len(unique_images)} å¼ ):\n")
            for i, img_path in enumerate(sorted(unique_images), 1):
                img_name = Path(img_path).name
                print(f"{i}. {img_name}")
                print(f"   è·¯å¾„: {img_path}")
                
                # æ˜¾ç¤ºå…ƒæ•°æ®
                for metadata in results['metadatas']:
                    path = metadata.get('path') or metadata.get('source')
                    if path == img_path:
                        if 'size' in metadata:
                            print(f"   å¤§å°: {metadata['size']}")
                        if 'format' in metadata:
                            print(f"   æ ¼å¼: {metadata['format']}")
                        break
                print()
        else:
            print("æ•°æ®åº“ä¸­æ²¡æœ‰å›¾ç‰‡")
    except Exception as e:
        print(f"âŒ åˆ—å‡ºå›¾ç‰‡æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = setup_argparse()
    
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("=" * 60)
    print("Local Multimodal AI Assistant")
    print("=" * 60)
    
    try:
        text_processor = TextProcessor()
        image_processor = ImageProcessor()
        vector_db = VectorDB()
        classifier = Classifier()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–ç»„ä»¶å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ ¹æ®å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.command == "add_paper":
        handle_add_paper(args, text_processor, vector_db, classifier)
    
    elif args.command == "search_paper":
        handle_search_paper(args, text_processor, vector_db)
    
    elif args.command == "search_image":
        handle_search_image(args, image_processor, vector_db)
    
    elif args.command == "add_image":
        handle_add_image(args, image_processor, vector_db)
    
    elif args.command == "add_images":
        handle_add_images(args, image_processor, vector_db)
    
    elif args.command == "organize":
        handle_organize(args, classifier)
    
    elif args.command == "list_papers":
        handle_list_papers(vector_db)
    
    elif args.command == "list_images":
        handle_list_images(vector_db)
    
    elif args.command == "clear_db":
        if args.confirm:
            print("æ­£åœ¨æ¸…é™¤æ•°æ®åº“...")
            if hasattr(vector_db, 'clear_database'):
                if vector_db.clear_database():
                    print("âœ… æ•°æ®åº“å·²æ¸…ç©º")
                else:
                    print("âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥")
            else:
                print("âŒ clear_database æ–¹æ³•æœªå®ç°")
        else:
            print("âš ï¸  è­¦å‘Šï¼šè¿™å°†åˆ é™¤æ‰€æœ‰ç´¢å¼•æ•°æ®ï¼")
            print("ä½¿ç”¨ --confirm å‚æ•°ç¡®è®¤æ“ä½œ")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
