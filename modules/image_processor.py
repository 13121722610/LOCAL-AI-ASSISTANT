# image_processor.py
import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
from transformers import CLIPProcessor, CLIPModel
from typing import List
import config

class ImageProcessor:
    """å›¾åƒå¤„ç†æ¨¡å—"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = CLIPModel.from_pretrained(config.IMAGE_MODEL_NAME).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(config.IMAGE_MODEL_NAME)
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
        print(f"Image model loaded on {self.device}")
    
    def encode_image(self, image_path: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªå›¾åƒä¸ºå‘é‡ï¼ˆL2å½’ä¸€åŒ–ï¼‰"""
        try:
            image = Image.open(image_path).convert("RGB")
            inputs = self.processor(images=image, return_tensors="pt", padding=True)
            
            with torch.no_grad():
                image_features = self.model.get_image_features(
                    inputs["pixel_values"].to(self.device)
                )
                # L2 å½’ä¸€åŒ– - å…³é”®ä¿®å¤ï¼
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embedding = image_features.cpu().numpy()[0]
            
            return embedding
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
            return np.zeros(config.IMAGE_EMBEDDING_DIM)
    
    def encode_images(self, image_paths: List[str]) -> List[np.ndarray]:
        """æ‰¹é‡ç¼–ç å›¾åƒï¼ˆL2å½’ä¸€åŒ–ï¼‰"""
        embeddings = []
        for path in image_paths:
            embedding = self.encode_image(path)
            embeddings.append(embedding)
        return embeddings
    
    def encode_text_for_image_search(self, text: str) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ç”¨äºå›¾åƒæœç´¢ï¼ˆL2å½’ä¸€åŒ–ï¼‰"""
        inputs = self.processor(
            text=[text],
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        with torch.no_grad():
            text_features = self.model.get_text_features(
                inputs["input_ids"].to(self.device)
            )
            # L2 å½’ä¸€åŒ– - å…³é”®ä¿®å¤ï¼
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            embedding = text_features.cpu().numpy()[0]
        
        return embedding
    
    def compute_similarity(self, image_embedding: np.ndarray, text_embedding: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_vec = np.array(image_embedding)
        text_vec = np.array(text_embedding)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦ = (AÂ·B) / (||A|| * ||B||)
        # ç”±äºå·²ç»å½’ä¸€åŒ–ï¼Œ||A|| = ||B|| = 1ï¼Œæ‰€ä»¥ç®€åŒ–ä¸ºç‚¹ç§¯
        similarity = np.dot(img_vec, text_vec)
        
        # ç¡®ä¿åœ¨-1åˆ°1ä¹‹é—´ï¼ˆç†è®ºä¸Šåº”è¯¥åœ¨0-1ä¹‹é—´ï¼Œå› ä¸ºç‰¹å¾éƒ½æ˜¯æ­£çš„ï¼‰
        similarity = max(-1.0, min(1.0, similarity))
        
        return float(similarity)
    
    def test_normalization(self):
        """æµ‹è¯•å½’ä¸€åŒ–æ•ˆæœ"""
        print("\nğŸ”§ æµ‹è¯•ç‰¹å¾å½’ä¸€åŒ–...")
        
        # æµ‹è¯•æ–‡æœ¬ç¼–ç å½’ä¸€åŒ–
        test_text = "a photo of sunset"
        text_emb = self.encode_text_for_image_search(test_text)
        text_norm = np.linalg.norm(text_emb)
        print(f"æ–‡æœ¬ç‰¹å¾ '{test_text}' çš„èŒƒæ•°: {text_norm:.6f}")
        
        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæµ‹è¯•å›¾ç‰‡ç¼–ç å½’ä¸€åŒ–
        import os
        if os.path.exists("1.png"):
            img_emb = self.encode_image("1.png")
            img_norm = np.linalg.norm(img_emb)
            print(f"å›¾ç‰‡ '1.png' çš„èŒƒæ•°: {img_norm:.6f}")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.compute_similarity(img_emb, text_emb)
            print(f"å›¾ç‰‡ä¸æ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
        
        print("âœ… å½’ä¸€åŒ–æµ‹è¯•å®Œæˆ")

